{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYLxxWlZdt4n"
      },
      "source": [
        "citc: rl\n",
        "\n",
        "blaze-bin/experimental/users/ppiech/rl/notebook.par \u003e /dev/null 2\u003e\u00261 \u0026 disown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBJQaJaK_UMg"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "from absl import logging\n",
        "import sys\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import base64\n",
        "import imageio\n",
        "import ffmpeg\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import reverb\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "import tf_agents.environments.utils \n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import greedy_policy\n",
        "from tf_agents.policies import policy_saver\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common\n",
        "\n",
        "from colabtools import adhoc_import\n",
        "\n",
        "with adhoc_import.Google3CitcClient('rl'):\n",
        "  # Now we can import pure-python code from anywhere in google3, including experimental.\n",
        "  #\n",
        "  # NOTE: anything with a (transitive) build-requiring dependency such as C++\n",
        "  # code, protocol buffers, etc. that isn't already built in to the colab_binary\n",
        "  # requires a build_targets= param. See go/adhoc-build below.\n",
        "  from google3.pyglib import gfile\n",
        "  from google3.experimental.users.ppiech.spaceship_learn.spaceship_env import SpaceshipEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUsaAktQ3_nG"
      },
      "outputs": [],
      "source": [
        "def pipe_to_ffmpeg(filename):\n",
        "  args = (\n",
        "        ffmpeg\n",
        "        .input('pipe:')\n",
        "        .output(filename)\n",
        "        .overwrite_output()\n",
        "        .compile()\n",
        "    )\n",
        "  return subprocess.Popen(args, stdin=subprocess.PIPE)\n",
        "\n",
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  \u003cvideo width=\"640\" height=\"480\" controls\u003e\n",
        "    \u003csource src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"\u003e\n",
        "  Your browser does not support the video tag.\n",
        "  \u003c/video\u003e'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfjhZ0zcQPrI"
      },
      "outputs": [],
      "source": [
        "num_iterations = 40000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 1000  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = 10 # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 5000  # @param {type:\"integer\"}\n",
        "\n",
        "log_interval = 1000  # @param {type:\"integer\"}\n",
        "train_checkpoint_interval=10000 # @param {type:\"integer\"}\n",
        "policy_checkpoint_interval=5000 # @param {type:\"integer\"}\n",
        "rb_checkpoint_interval=2000 # @param {type:\"integer\"}\n",
        "keep_rb_checkpoint=False # @param {type:\"boolean\"}\n",
        "train_sequence_length=1 # @param {type:\"integer\"}\n",
        "summary_interval=1000 # @param {type:\"integer\"}\n",
        "summaries_flush_secs=10 # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCIMR8Rm9i8N"
      },
      "outputs": [],
      "source": [
        "root_dir = os.path.expanduser('log')\n",
        "train_dir = os.path.join(root_dir, 'train')\n",
        "eval_dir = os.path.join(root_dir, 'eval')\n",
        "saved_model_dir = os.path.join(root_dir, 'policy_saved_model')\n",
        "if not tf.io.gfile.exists(saved_model_dir):\n",
        "  tf.io.gfile.makedirs(saved_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3eSyKGdQSj9"
      },
      "outputs": [],
      "source": [
        "def test_env():\n",
        "  # env_name = 'CartPole-v0'\n",
        "  # env = suite_gym.load(env_name)\n",
        "\n",
        "  env = SpaceshipEnv()\n",
        "  tf_agents.environments.utils.validate_py_environment(env, episodes=5)\n",
        "\n",
        "#test_env()\n",
        "\n",
        "env = SpaceshipEnv()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MnplGDZQV-o"
      },
      "outputs": [],
      "source": [
        "def snapshot_render():\n",
        "  env.reset()\n",
        "  print(env.time_step_spec())\n",
        "  PIL.Image.fromarray(env.render())\n",
        "\n",
        "#snapshot_render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eTVfZOxUccR"
      },
      "outputs": [],
      "source": [
        "def print_env_specs():\n",
        "  print('Observation Spec:')\n",
        "  print(env.time_step_spec().observation)\n",
        "  print('Reward Spec:')\n",
        "  print(env.time_step_spec().reward)\n",
        "  print('Action Spec:')\n",
        "  print(env.action_spec())\n",
        "\n",
        "#print_env_specs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOMJq1g0bqkm"
      },
      "outputs": [],
      "source": [
        "def capture_random_video():\n",
        "  video_filename = '/tmp/starship_env_random.mp4'\n",
        "\n",
        "  video_pipe = pipe_to_ffmpeg(video_filename)\n",
        "\n",
        "  env.reset()\n",
        "  for _ in range(100):\n",
        "    env.step(random.randint(-1, 1))\n",
        "    image = PIL.Image.fromarray(env.render())\n",
        "    image.save(video_pipe.stdin, 'BMP')\n",
        "\n",
        "  video_pipe.stdin.close()\n",
        "  video_pipe.wait()\n",
        "\n",
        "  embed_mp4(video_filename)\n",
        "\n",
        "#capture_random_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYkcen3qUnHg"
      },
      "outputs": [],
      "source": [
        "def print_time_step():\n",
        "  time_step = env.reset()\n",
        "  print('Time step:')\n",
        "  print(time_step)\n",
        "\n",
        "  action = np.array(1, dtype=np.int32)\n",
        "\n",
        "  next_time_step = env.step(action)\n",
        "  print('Next time step:')\n",
        "  print(next_time_step)\n",
        "\n",
        "#print_time_step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6sUm8GHUqH-"
      },
      "outputs": [],
      "source": [
        "# train_py_env = suite_gym.load(env_name)\n",
        "# eval_py_env = suite_gym.load(env_name)\n",
        "train_py_env = SpaceshipEnv()\n",
        "eval_py_env = SpaceshipEnv()\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B7k3z_KU0ry"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Policy\n",
        "#\n",
        "fc_layer_params = (200, 100)\n",
        "action_tensor_spec = tensor_spec.from_spec(train_py_env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S89daYCRqHh_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H-fyX01U3b6"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Agent\n",
        "#\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0, dtype=tf.int64)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()\n",
        "\n",
        "policy_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=os.path.join(train_dir, 'policy'),\n",
        "    max_to_keep=None,\n",
        "    policy=agent.policy,\n",
        "    global_step=agent.train_step_counter)\n",
        "\n",
        "saved_model = policy_saver.PolicySaver(\n",
        "    greedy_policy.GreedyPolicy(agent.policy), train_step=train_step_counter)\n",
        "\n",
        "def save_policy(global_step_value):\n",
        "    \"\"\"Saves policy using both checkpoint saver and saved model.\"\"\"\n",
        "    policy_checkpointer.save(global_step=global_step_value)\n",
        "    saved_model_path = os.path.join(\n",
        "        saved_model_dir, 'policy_' + ('%d' % global_step_value).zfill(8))\n",
        "    saved_model.save(saved_model_path)\n",
        "\n",
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyf7UIXZVR09"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Replay buffer\n",
        "#\n",
        "\n",
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "# table = reverb.Table(\n",
        "#     table_name,\n",
        "#     max_size=replay_buffer_max_length,\n",
        "#     sampler=reverb.selectors.Uniform(),\n",
        "#     remover=reverb.selectors.Fifo(),\n",
        "#     rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "#     signature=replay_buffer_signature)\n",
        "\n",
        "# reverb_server = reverb.Server([table])\n",
        "\n",
        "# replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "#     agent.collect_data_spec,\n",
        "#     table_name=table_name,\n",
        "#     sequence_length=2,\n",
        "#     local_server=reverb_server)\n",
        "\n",
        "# rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "#   replay_buffer.py_client,  #not py_client\n",
        "#   table_name,\n",
        "#   sequence_length=2)\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    agent.collect_data_spec, \n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "print('RB capacity: %i' % replay_buffer.capacity)\n",
        "rb_observer = replay_buffer.add_batch\n",
        "\n",
        "rb_ckpt_dir = os.path.join(train_dir, 'replay_buffer')\n",
        "rb_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=rb_ckpt_dir, max_to_keep=1, replay_buffer=replay_buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o2p1Ffak65A"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Train Metrics\n",
        "#\n",
        "\n",
        "train_metrics = [\n",
        "    tf_metrics.NumberOfEpisodes(),\n",
        "    tf_metrics.EnvironmentSteps(),\n",
        "    tf_metrics.AverageReturnMetric(batch_size=train_env.batch_size),\n",
        "    tf_metrics.AverageEpisodeLengthMetric(batch_size=train_env.batch_size),\n",
        "    tf_metrics.ChosenActionHistogram(),\n",
        "]\n",
        "\n",
        "train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=train_dir,\n",
        "    max_to_keep=1,\n",
        "    agent=agent,\n",
        "    global_step=agent.train_step_counter,\n",
        "    metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsSuH7GHVlBH"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Driver\n",
        "#\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec(), \n",
        "                                                info_spec=agent.collect_policy.info_spec)\n",
        "\n",
        "random_policy = agent.collect_policy\n",
        "\n",
        "agent_observers = [replay_buffer.add_batch]\n",
        "\n",
        "DynamicStepDriver(\n",
        "    train_env, agent.collect_policy, [rb_observer], num_steps=1000).run()\n",
        "# driver.num_steps = 1000\n",
        "# driver.run()\n",
        "\n",
        "# py_driver.PyDriver(\n",
        "#     env,\n",
        "#     py_tf_eager_policy.PyTFEagerPolicy(\n",
        "#       random_policy, use_tf_function=True),\n",
        "#     [rb_observer],\n",
        "#     max_steps=initial_collect_steps).run(train_py_env.reset())\n",
        "\n",
        "# # Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRR_FUo686NZ"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Eval Metrics\n",
        "#\n",
        "\n",
        "eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
        "    eval_dir, \n",
        "    flush_millis=summaries_flush_secs * 1000\n",
        ")\n",
        "\n",
        "eval_metrics = [\n",
        "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
        "    tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes),\n",
        "]\n",
        "\n",
        "eval_policy =  greedy_policy.GreedyPolicy(agent.policy)\n",
        " \n",
        "metric_utils.eager_compute(\n",
        "    eval_metrics,\n",
        "    eval_env,\n",
        "    eval_policy,\n",
        "    num_episodes=num_eval_episodes,\n",
        "    summary_writer=eval_summary_writer,\n",
        "    train_step=0\n",
        ")\n",
        "\n",
        "metric_utils.log_metrics(eval_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eruBOqKgVxNl"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Train Loop\n",
        "#\n",
        "\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_env.reset()\n",
        "\n",
        "collect_driver = DynamicStepDriver(\n",
        "    train_env, agent.collect_policy, [rb_observer], \n",
        "    num_steps=collect_steps_per_iteration)\n",
        "\n",
        "timed_at_step = agent.train_step_counter.numpy()\n",
        "time_acc = 0\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step_tf = agent.train_step_counter\n",
        "  step = step_tf.numpy()\n",
        "\n",
        "  time_acc += time.time() - start_time\n",
        "\n",
        "  for train_metric in train_metrics:\n",
        "    train_metric.tf_summaries(\n",
        "        train_step=agent.train_step_counter, step_metrics=train_metrics[:2])\n",
        "\n",
        "  if step % train_checkpoint_interval == 0:\n",
        "    train_checkpointer.save(global_step=step)\n",
        "\n",
        "  if step % policy_checkpoint_interval == 0:\n",
        "    save_policy(step)\n",
        "\n",
        "  if step % rb_checkpoint_interval == 0:\n",
        "    rb_checkpointer.save(global_step=step)\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = %d, loss = %f' % (step, train_loss.numpy()))\n",
        "    steps_per_sec = (step- timed_at_step) * collect_steps_per_iteration / time_acc\n",
        "    print('%.3f steps/sec' % steps_per_sec)\n",
        "    tf.compat.v2.summary.scalar(\n",
        "        name='env_steps_per_sec', data=steps_per_sec, step=step_tf)\n",
        "    timed_at_step = step\n",
        "    time_acc = 0\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    metric_utils.eager_compute(\n",
        "      eval_metrics,\n",
        "      eval_env,\n",
        "      eval_policy,\n",
        "      num_episodes=num_eval_episodes,\n",
        "      summary_writer=eval_summary_writer,\n",
        "      train_step=step\n",
        "    )\n",
        "\n",
        "    metric_utils.log_metrics(eval_metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWkUQqCVXfX_"
      },
      "outputs": [],
      "source": [
        "metric_utils.eager_compute(\n",
        "      eval_metrics,\n",
        "      eval_env,\n",
        "      eval_policy,\n",
        "      num_episodes=num_eval_episodes,\n",
        "      summary_writer=eval_summary_writer,\n",
        "      train_step=step\n",
        "    )\n",
        "\n",
        "print (eval_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHqJccOdLJ1v"
      },
      "outputs": [],
      "source": [
        "train_metrics = [\n",
        "    tf_metrics.NumberOfEpisodes(),\n",
        "    tf_metrics.EnvironmentSteps(),\n",
        "    tf_metrics.AverageReturnMetric(batch_size=train_env.batch_size),\n",
        "    tf_metrics.AverageEpisodeLengthMetric(batch_size=train_env.batch_size),\n",
        "    tf_metrics.ChosenActionHistogram(),\n",
        "]\n",
        "\n",
        "train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=train_dir,\n",
        "    max_to_keep=1,\n",
        "    agent=agent,\n",
        "    global_step=agent.train_step_counter,\n",
        "    metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coTQ4yfems3N"
      },
      "outputs": [],
      "source": [
        "%load_ext google3.learning.brain.tensorboard.notebook.extension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF2rXhJwQtJZ"
      },
      "outputs": [],
      "source": [
        "from google3.third_party.tensorboard import notebook\n",
        "notebook.list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnEapdy3n1rG"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=\"log\" --port=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKCoaxQZWICF"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  \u003cvideo width=\"640\" height=\"480\" controls\u003e\n",
        "    \u003csource src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"\u003e\n",
        "  Your browser does not support the video tag.\n",
        "  \u003c/video\u003e'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)\n",
        "\n",
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return embed_mp4(filename)\n",
        "\n",
        "create_policy_eval_video(agent.policy, \"trained-agent\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "spaceship_tf_agents",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/users/ppiech/rl/Untitled0.ipynb?workspaceId=ppiech:rl::citc",
          "timestamp": 1680967789605
        },
        {
          "file_id": "13jtLPtJQlVWQkWlOxjGpxKOqbs5rwIO-",
          "timestamp": 1679285442760
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
